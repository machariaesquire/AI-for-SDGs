{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14oZA45mVa3fpghVi6J4-JAyFNcwjcMoA",
      "authorship_tag": "ABX9TyOi4lypBe5J5avhlFLt37+i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machariaesquire/AI-for-SDGs/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tkjxJCnx6Ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e2e230-f05b-4470-a2b9-95a410862026"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Algerian_forest_fires_dataset.csv  fire_archive_M6_96619.csv\n",
            "amazon-rainforest\t\t   inpe_brazilian_amazon_fires_1999_2019.csv\n",
            "def_area_2004_2019.csv\tel_nino_la_nina_1999_2019.csv  inpe_brazilian_amazon_fires_1999_2019.csv\n",
            "   year  month        state   latitude  longitude  firespots\n",
            "0  1999      1     AMAZONAS  -2.371113 -59.899933          3\n",
            "1  1999      1     MARANHAO  -2.257395 -45.487831         36\n",
            "2  1999      1  MATO GROSSO -12.660633 -55.057989         18\n",
            "3  1999      1         PARA  -2.474820 -48.546967         87\n",
            "4  1999      1     RONDONIA -12.861700 -60.513100          1\n",
            "Found 8320 images belonging to 1 classes.\n",
            "Found 2080 images belonging to 1 classes.\n",
            "Epoch 1/10\n",
            "260/260 [==============================] - 65s 245ms/step - loss: 0.0033 - accuracy: 0.9972 - val_loss: 1.9591e-29 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "260/260 [==============================] - 64s 248ms/step - loss: 3.6834e-20 - accuracy: 1.0000 - val_loss: 1.9591e-29 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "260/260 [==============================] - 62s 239ms/step - loss: 3.6834e-20 - accuracy: 1.0000 - val_loss: 1.9591e-29 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "260/260 [==============================] - 63s 242ms/step - loss: 3.6834e-20 - accuracy: 1.0000 - val_loss: 1.9591e-29 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "260/260 [==============================] - 64s 245ms/step - loss: 3.6834e-20 - accuracy: 1.0000 - val_loss: 1.9591e-29 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "260/260 [==============================] - 64s 248ms/step - loss: 3.6834e-20 - accuracy: 1.0000 - val_loss: 1.9591e-29 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "260/260 [==============================] - 64s 247ms/step - loss: 3.6834e-20 - accuracy: 1.0000 - val_loss: 1.9591e-29 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "260/260 [==============================] - 59s 228ms/step - loss: 3.6834e-20 - accuracy: 1.0000 - val_loss: 1.9591e-29 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "260/260 [==============================] - 63s 240ms/step - loss: 3.6834e-20 - accuracy: 1.0000 - val_loss: 1.9591e-29 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "260/260 [==============================] - 68s 262ms/step - loss: 3.6834e-20 - accuracy: 1.0000 - val_loss: 1.9591e-29 - val_accuracy: 1.0000\n",
            "65/65 [==============================] - 6s 89ms/step - loss: 1.9591e-29 - accuracy: 1.0000\n",
            "CNN Validation Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify file path\n",
        "!ls /content/drive/MyDrive/datasets\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_path = '/content/drive/MyDrive/archive (2).zip'  # Update with the zip file path\n",
        "extract_path = '/content/drive/MyDrive/datasets/amazon-rainforest'\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/archive (5).zip'\n",
        "extract_path = '/content/forest_images'\n",
        "\n",
        "# Create extract directory if not exists\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Verify extracted files\n",
        "!ls /content/drive/MyDrive/datasets/amazon-rainforest\n",
        "\n",
        "\n",
        "# Setup paths for datasets\n",
        "satellite_data_path = '/content/drive/MyDrive/datasets/amazon-rainforest'\n",
        "forest_fires_data_path = '/content/drive/MyDrive/datasets/Algerian_forest_fires_dataset.csv'\n",
        "news_data_path = '/content/drive/MyDrive/datasets/fire_archive_M6_96619.csv'\n",
        "\n",
        "# Load the Brazilian Amazon Fires dataset (your custom dataset)\n",
        "brazilian_amazon_fires_path = '/content/drive/MyDrive/datasets/inpe_brazilian_amazon_fires_1999_2019.csv'\n",
        "brazilian_amazon_fires_data = pd.read_csv(brazilian_amazon_fires_path)\n",
        "\n",
        "# Check if data is loaded correctly\n",
        "print(brazilian_amazon_fires_data.head())\n",
        "\n",
        "# Load and preprocess the extracted images\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    extract_path,\n",
        "    target_size=(64, 64),  # Adjust based on your dataset\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    extract_path,\n",
        "    target_size=(64, 64),  # Adjust based on your dataset\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Define the CNN model\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the CNN model\n",
        "cnn_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "# Evaluate the CNN model\n",
        "cnn_loss, cnn_accuracy = cnn_model.evaluate(validation_generator)\n",
        "print(f'CNN Validation Accuracy: {cnn_accuracy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess forest fires data\n",
        "forest_data = pd.read_csv(forest_fires_data_path)\n",
        "\n",
        "# Inspect the column names to ensure correct references\n",
        "print(forest_data.columns)\n",
        "\n",
        "# Strip any leading or trailing spaces from column names\n",
        "forest_data.columns = forest_data.columns.str.strip()\n",
        "\n",
        "# Convert feature columns to numeric, forcing non-numeric values to NaN\n",
        "forest_data['Temperature'] = pd.to_numeric(forest_data['Temperature'], errors='coerce')\n",
        "forest_data['RH'] = pd.to_numeric(forest_data['RH'], errors='coerce')\n",
        "forest_data['Ws'] = pd.to_numeric(forest_data['Ws'], errors='coerce')\n",
        "forest_data['Rain'] = pd.to_numeric(forest_data['Rain'], errors='coerce')\n",
        "\n",
        "# Convert target column to numeric, forcing non-numeric values to NaN\n",
        "forest_data['FWI'] = pd.to_numeric(forest_data['FWI'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in features and target columns\n",
        "forest_data = forest_data.dropna(subset=['Temperature', 'RH', 'Ws', 'Rain', 'FWI'])\n",
        "\n",
        "# Select features and target\n",
        "features = forest_data[['Temperature', 'RH', 'Ws', 'Rain']]\n",
        "target = forest_data['FWI']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
        "print(f'Random Forest Mean Absolute Error: {rf_mae}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWkO2nH5lUtY",
        "outputId": "414fc266-5160-4392-a938-548eb5b49d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['day', 'month', 'year', 'Temperature', ' RH', ' Ws', 'Rain ', 'FFMC',\n",
            "       'DMC', 'DC', 'ISI', 'BUI', 'FWI', 'Classes  '],\n",
            "      dtype='object')\n",
            "Random Forest Mean Absolute Error: 2.0822504859086495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the uploaded zip file\n",
        "zip_path = '/content/drive/MyDrive/archive (7).zip'\n",
        "extract_path = '/content/sentiment_analysis_data'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Verify the extracted files\n",
        "!ls /content/sentiment_analysis_data\n",
        "\n",
        "# Load and preprocess the text data\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming the dataset is in a CSV file within the extracted folder\n",
        "data_file = '/content/drive/MyDrive/datasets/guardian_environment_news.csv'  # Update this to the actual CSV file name\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(data_file)\n",
        "\n",
        "# Inspect the first few rows of the dataset\n",
        "print(data.head())\n",
        "print(data.columns)\n",
        "\n",
        "# Using 'Article Text' for the text data\n",
        "X = data['Article Text'].fillna('')  # Use the 'Article Text' column for the text data\n",
        "\n",
        "# For demonstration, create a simple sentiment label based on the presence of specific keywords\n",
        "keywords = ['environment', 'climate', 'sustainability', 'conservation', 'biodiversity']\n",
        "y = X.apply(lambda text: 1 if any(keyword in text.lower() for keyword in keywords) else 0)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "X_padded = pad_sequences(X_sequences, maxlen=100)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define and train the LSTM model for sentiment analysis\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    Embedding(10000, 128, input_length=100),\n",
        "    LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model\n",
        "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict and evaluate\n",
        "lstm_predictions = lstm_model.predict(X_test)\n",
        "lstm_predictions = (lstm_predictions > 0.5).astype(int)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "lstm_accuracy = accuracy_score(y_test, lstm_predictions)\n",
        "print(f'LSTM Sentiment Analysis Accuracy: {lstm_accuracy}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls1F9hHHlZ1Q",
        "outputId": "8bfeb963-d907-4797-e5f1-fc2efe36ecd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "guardian_environment_news.csv\n",
            "                                               Title  \\\n",
            "0   Liz Truss ‘will approve more oil drilling if ...   \n",
            "1  Renewed Highland golf course plan has environm...   \n",
            "2   Visiting green spaces deters mental health dr...   \n",
            "3  Bought too much red cabbage? Turn it into a fe...   \n",
            "4  ‘This year has been very good’: readers’ UK bu...   \n",
            "\n",
            "                                          Intro Text  \\\n",
            "0  Tory leadership candidate criticised by campai...   \n",
            "1  Scottish government rejected a new links at Co...   \n",
            "2  Positive effects were stronger among those rep...   \n",
            "3  This fantastic vegan centrepiece makes full us...   \n",
            "4  Readers share their favourite sightings over t...   \n",
            "\n",
            "                            Authors  \\\n",
            "0    ['Rob Davies', '@ByRobDavies']   \n",
            "1  ['Ewan Murray', '@mrewanmurray']   \n",
            "2  ['Damien Gayle', '@damiengayle']   \n",
            "3                      ['Tom Hunt']   \n",
            "4              ['Guardian readers']   \n",
            "\n",
            "                                        Article Text Date Published  \n",
            "0  Liz Truss will sign off on a push for more oil...     2022-08-30  \n",
            "1  It is an area so tranquil that the notion of b...     2021-03-22  \n",
            "2  Visits to parks, community gardens and other u...     2023-01-17  \n",
            "3  I devised today’s nut roast for Oddbox, a veg ...     2023-12-22  \n",
            "4  ‘Constant companions to our gardening’A peacoc...     2023-12-19  \n",
            "Index(['Title', 'Intro Text', 'Authors', 'Article Text', 'Date Published'], dtype='object')\n",
            "Epoch 1/5\n",
            "602/602 [==============================] - 257s 419ms/step - loss: 0.4448 - accuracy: 0.8099 - val_loss: 0.3946 - val_accuracy: 0.8383\n",
            "Epoch 2/5\n",
            "602/602 [==============================] - 231s 384ms/step - loss: 0.3388 - accuracy: 0.8594 - val_loss: 0.3752 - val_accuracy: 0.8133\n",
            "Epoch 3/5\n",
            "602/602 [==============================] - 229s 381ms/step - loss: 0.2605 - accuracy: 0.8887 - val_loss: 0.3855 - val_accuracy: 0.8306\n",
            "Epoch 4/5\n",
            "602/602 [==============================] - 227s 377ms/step - loss: 0.1822 - accuracy: 0.9244 - val_loss: 0.4515 - val_accuracy: 0.8254\n",
            "Epoch 5/5\n",
            "602/602 [==============================] - 226s 376ms/step - loss: 0.1215 - accuracy: 0.9527 - val_loss: 0.5308 - val_accuracy: 0.7975\n",
            "188/188 [==============================] - 8s 42ms/step\n",
            "LSTM Sentiment Analysis Accuracy: 0.792581503659348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of Results\n",
        "print(f'CNN Validation Accuracy: {cnn_accuracy}')\n",
        "print(f'Random Forest Mean Absolute Error: {rf_mae}')\n",
        "print(f'LSTM Sentiment Analysis Accuracy: {lstm_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwSBu152ldBu",
        "outputId": "be8e37ab-fbe2-47c4-ee2c-c1e0a1f494f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Validation Accuracy: 1.0\n",
            "Random Forest Mean Absolute Error: 2.0822504859086495\n",
            "LSTM Sentiment Analysis Accuracy: 0.792581503659348\n"
          ]
        }
      ]
    }
  ]
}